{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from api_key import api_key\n",
    "from openai import OpenAI\n",
    "from print_streaming_response import print_streaming_response\n",
    "\n",
    "endpoint = \"https://conductor.arcee.ai/v1\"\n",
    "\n",
    "model = \"auto-reasoning\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of models from the API\n",
    "models = client.models.list()\n",
    "\n",
    "# Iterate over the models and print a sorted list\n",
    "for m in sorted(models, key=lambda x: x.id):\n",
    "    print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Write a friendly welcome message for a new user of Arcee Conductor, \n",
    "            Arcee AI's inference platform. Conductor intelligently routes your prompt to the best model, to efficiently deliver precise results, \n",
    "            for any task. Get started with $20 free credit. Learn more at https://arcee.ai/product/conductor.\n",
    "            \"\"\",\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print_streaming_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"\n",
    "            Suggest some productivity tips for frequent intercontinental flights between Paris and San Francisco.\n",
    "            I usually fly mid-day or early afternoon, and I need to be productive on the plane. I work on a Mac laptop\n",
    "            Wi-fi may not be available, so I need to be able to work offline.\n",
    "            My work involves reading, writing, and coding.\n",
    "            \"\"\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "\n",
    "print_streaming_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"I will attend two industry events in June. \n",
    "            One from June 9-13 in Washington DC, and one from June 10-12 in San Diego.\n",
    "            I only need to spend the full June 10 in DC, and I want to fly in the day before.\n",
    "            I will initially fly from Paris, France.\n",
    "            What are the best options for my itinerary?\n",
    "            \"\"\",\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=16384\n",
    ")\n",
    "\n",
    "\n",
    "print_streaming_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Explain when I should use put options to protect my portfolio of US stocks \n",
    "            against a market downturn. Show me an example.\"\"\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "\n",
    "print_streaming_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "def print_streaming_response(response):\n",
    "    num_tokens = 0\n",
    "    content = \"\"\n",
    "    model_id = None\n",
    "    \n",
    "    for message in response:\n",
    "        if len(message.choices) > 0:\n",
    "            # Capture model ID from the first chunk if available\n",
    "            if model_id is None and hasattr(message, 'model'):\n",
    "                model_id = message.model\n",
    "                \n",
    "            num_tokens += 1\n",
    "            chunk = message.choices[0].delta.content\n",
    "            if chunk:\n",
    "                content += chunk\n",
    "                clear_output(wait=True)\n",
    "                display(Markdown(content))\n",
    "    \n",
    "    print(f\"\\n\\nNumber of tokens: {num_tokens}\")\n",
    "    if model_id:\n",
    "        print(f\"Model ID: {model_id}\")\n",
    "    else:\n",
    "        print(\"Model ID not available in response\")\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"auto\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Make this function more pythonic.\n",
    "            Explain possible tradeoffs between memory efficiency, execution speed, and maintainabilty.\n",
    "            Suggest what you think is the best approach.\n",
    "            \n",
    "            Code: {code}\"\"\",\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print_streaming_response(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
