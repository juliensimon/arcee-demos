{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from api_key import api_key\n",
    "\n",
    "endpoint=\"https://models.arcee.ai/v1\"\n",
    "\n",
    "model=\"coder\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=api_key,\n",
    "    http_client=httpx.Client(http2=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_streaming_response(response):\n",
    "    num_tokens=0\n",
    "    for message in response:\n",
    "        if len(message.choices) > 0:\n",
    "            num_tokens+=1\n",
    "            print(message.choices[0].delta.content, end=\"\")\n",
    "    print(f\"\\n\\nNumber of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Knowledge Distillation is a technique used to transfer knowledge from a large, complex model (teacher) to a smaller, simpler model (student). Two common methods of knowledge distillation in the context of transformer models like BERT are **logit-based distillation** and **hidden state distillation**.\n",
      "\n",
      "### Logit-Based Distillation\n",
      "\n",
      "In logit-based distillation, the knowledge is transferred by matching the teacher's and student's logits (the pre-softmax activations) rather than their final softmax probabilities. The idea is to train the student to produce logits that are close to the teacher's logits using a temperature-scaled cross-entropy loss.\n",
      "\n",
      "#### Example with PyTorch\n",
      "\n",
      "Here's an example of logit-based distillation using BERT-Large as the teacher and BERT-Base as the student.\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
      "\n",
      "# Load teacher and student models\n",
      "teacher_model = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=2)\n",
      "student_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
      "\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "\n",
      "# Dummy input data\n",
      "texts = [\"I love machine learning!\", \"I hate spam emails.\"]\n",
      "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
      "\n",
      "# Training parameters\n",
      "num_epochs = 1\n",
      "temperature = 2.0\n",
      "alpha = 0.5  # Weight for original loss vs distillation loss\n",
      "optimizer = optim.Adam(student_model.parameters(), lr=1e-5)\n",
      "\n",
      "# Move to GPU if available\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "teacher_model.to(device)\n",
      "student_model.to(device)\n",
      "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
      "\n",
      "# Training loop\n",
      "teacher_model.eval()\n",
      "student_model.train()\n",
      "\n",
      "for epoch in range(num_epochs):\n",
      "    optimizer.zero_grad()\n",
      "    \n",
      "    # Forward pass through the teacher model\n",
      "    with torch.no_grad():\n",
      "        teacher_logits = teacher_model(**inputs).logits\n",
      "    \n",
      "    # Forward pass through the student model\n",
      "    student_logits = student_model(**inputs).logits\n",
      "    \n",
      "    # Compute the original cross-entropy loss for student\n",
      "    loss_fn = nn.CrossEntropyLoss()\n",
      "    original_loss = loss_fn(student_logits, torch.tensor([1, 0]).to(device))  # Assuming labels are [1, 0]\n",
      "    \n",
      "    # Compute the knowledge distillation loss\n",
      "    soft_teacher_logits = torch.softmax(teacher_logits / temperature, dim=1)\n",
      "    distillation_loss = nn.KLDivLoss(reduction=\"batchmean\")(torch.log_softmax(student_logits / temperature, dim=1), soft_teacher_logits)\n",
      "    \n",
      "    # Combine the losses\n",
      "    total_loss = alpha * original_loss + (1 - alpha) * distillation_loss * temperature**2\n",
      "    \n",
      "    # Backward pass and optimization\n",
      "    total_loss.backward()\n",
      "    optimizer.step()\n",
      "    \n",
      "    print(f'Epoch {epoch+1}, Total Loss: {total_loss.item()}')\n",
      "```\n",
      "\n",
      "### Hidden State Distillation\n",
      "\n",
      "In hidden state distillation, knowledge is transferred by aligning the hidden states of the teacher and student models. This can be done at multiple layers, and various alignment techniques can be applied, such as Mean Squared Error (MSE).\n",
      "\n",
      "#### Example with PyTorch\n",
      "\n",
      "Here's an example of hidden state distillation using BERT-Large as the teacher and BERT-Base as the student.\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, BertConfig\n",
      "\n",
      "# Load teacher and student models\n",
      "teacher_model = BertModel.from_pretrained('bert-large-uncased')\n",
      "student_model = BertModel.from_pretrained('bert-base-uncased')\n",
      "\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "\n",
      "# Dummy input data\n",
      "texts = [\"I love machine learning!\", \"I hate spam emails.\"]\n",
      "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
      "\n",
      "# Training parameters\n",
      "num_epochs = 1\n",
      "alpha = 0.5  # Weight for original loss vs distillation loss\n",
      "optimizer = optim.Adam(student_model.parameters(), lr=1e-5)\n",
      "\n",
      "# Move to GPU if available\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "teacher_model.to(device)\n",
      "student_model.to(device)\n",
      "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
      "\n",
      "# Training loop\n",
      "teacher_model.eval()\n",
      "student_model.train()\n",
      "\n",
      "for epoch in range(num_epochs):\n",
      "    optimizer.zero_grad()\n",
      "    \n",
      "    # Forward pass through the teacher model\n",
      "    with torch.no_grad():\n",
      "        teacher_outputs = teacher_model(**inputs)\n",
      "        teacher_hidden_states = teacher_outputs.hidden_states\n",
      "    \n",
      "    # Forward pass through the student model\n",
      "    student_outputs = student_model(**inputs)\n",
      "    student_hidden_states = student_outputs.hidden_states\n",
      "    \n",
      "    # Compute the original cross-entropy loss for student\n",
      "    classifier = nn.Linear(student_model.config.hidden_size, 2).to(device)\n",
      "    student_logits = classifier(student_outputs.last_hidden_state[:, 0])  # Using the [CLS] token for classification\n",
      "    loss_fn = nn.CrossEntropyLoss()\n",
      "    original_loss = loss_fn(student_logits, torch.tensor([1, 0]).to(device))  # Assuming labels are [1, 0]\n",
      "    \n",
      "    # Compute the knowledge distillation loss\n",
      "    distillation_loss = 0\n",
      "    for teacher_hidden, student_hidden in zip(teacher_hidden_states, student_hidden_states):\n",
      "        distillation_loss += nn.MSELoss()(student_hidden, teacher_hidden)\n",
      "    distillation_loss /= len(teacher_hidden_states)\n",
      "    \n",
      "    # Combine the losses\n",
      "    total_loss = alpha * original_loss + (1 - alpha) * distillation_loss\n",
      "    \n",
      "    # Backward pass and optimization\n",
      "    total_loss.backward()\n",
      "    optimizer.step()\n",
      "    \n",
      "    print(f'Epoch {epoch+1}, Total Loss: {total_loss.item()}')\n",
      "```\n",
      "\n",
      "### Summary\n",
      "\n",
      "- **Logit-Based Distillation**: Focuses on aligning the logits before applying the softmax function. This is done with a temperature-scaled cross-entropy loss.\n",
      "- **Hidden State Distillation**: Focuses on aligning the hidden states at various layers of the transformer models using alignment techniques like MSE.\n",
      "\n",
      "Both methods are effective and can be used depending on the specific requirements and constraints of the problem.\n",
      "\n",
      "Number of tokens: 1352\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages=[\n",
    "      {'role': 'user', \n",
    "       'content': \"\"\"Explain the difference between logit-based distillation and hidden state distillation. Show an example for both with Pytorch code, \n",
    "       with BERT-Large as the teacher model, and BERT-Base as the student model.\n",
    "       \"\"\"\n",
    "      }   \n",
    "  ],\n",
    "  temperature=0.9,\n",
    "  stream=True,\n",
    "  max_tokens=16384\n",
    ")\n",
    "\n",
    "print_streaming_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Let's improve the provided code for printing a streaming response. Here's a revised version:\n",
      "\n",
      "```python\n",
      "def print_streaming_response(response):\n",
      "    num_tokens = 0\n",
      "    for message in response:\n",
      "        choices = message.get('choices', [])\n",
      "        for choice in choices:\n",
      "            delta_content = choice.get('delta', {}).get('content', '')\n",
      "            if delta_content:\n",
      "                num_tokens += 1\n",
      "                print(delta_content, end=\"\")\n",
      "    print(f\"\\n\\nNumber of tokens: {num_tokens}\")\n",
      "```\n",
      "\n",
      "### Explanation of Improvements:\n",
      "\n",
      "1. **Error Handling with `get`:**\n",
      "   - **Original Code:** `if len(message.choices) > 0:` assumes `message.choices` always exists, which can cause a `KeyError` if `choices` is missing.\n",
      "   - **Improved Code:** `choices = message.get('choices', [])` safely retrieves the `choices` list, defaulting to an empty list if `choices` is not present. This prevents potential `KeyError`.\n",
      "\n",
      "2. **Iterating Over Multiple Choices:**\n",
      "   - **Original Code:** Assumes there is only one choice (`message.choices[0]`), which is not always the case. Multiple choices may be present.\n",
      "   - **Improved Code:** Iterates over all choices in the `choices` list, ensuring that all available choices are processed.\n",
      "\n",
      "3. **Safe Access to `delta` and `content`:**\n",
      "   - **Original Code:** Directly accesses `message.choices[0].delta.content`, which could throw an `AttributeError` if either `delta` or `content` is missing.\n",
      "   - **Improved Code:** Uses `choice.get('delta', {}).get('content', '')` to safely access `content`, defaulting to an empty string if `delta` or `content` is not present.\n",
      "\n",
      "4. **Accurate Token Counting:**\n",
      "   - **Original Code:** Increments `num_tokens` for each message, but it may not accurately reflect the number of tokens if multiple choices or empty `delta.content` are present.\n",
      "   - **Improved Code:** Only increments `num_tokens` when `delta_content` is not empty, ensuring the token count is accurate.\n",
      "\n",
      "5. **Newlines for Better Readability:**\n",
      "   - Added two newlines before printing the token count for better readability in the output.\n",
      "\n",
      "These improvements make the function more robust, handle edge cases better, and ensure that it accurately reflects the contents and token counts of the streaming response.\n",
      "\n",
      "Number of tokens: 509\n"
     ]
    }
   ],
   "source": [
    "code_example = \"\"\"\n",
    "def print_streaming_response(response):\n",
    "    num_tokens=0\n",
    "    for message in response:\n",
    "        if len(message.choices) > 0:\n",
    "            num_tokens+=1\n",
    "            print(message.choices[0].delta.content, end=\"\")\n",
    "    print(f\"\\n\\nNumber of tokens: {num_tokens}\")\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages=[\n",
    "      {'role': 'user', \n",
    "       'content': f\"Improve the following code: {code_example}. Explain why your changes are an improvement.\"\n",
    "      }   \n",
    "  ],\n",
    "  temperature=0.9,\n",
    "  stream=True,\n",
    "  max_tokens=2048\n",
    ")\n",
    "\n",
    "print_streaming_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-openai-client",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
