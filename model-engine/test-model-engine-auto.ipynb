{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from api_key import api_key\n",
    "from openai import OpenAI\n",
    "from print_streaming_response import print_streaming_response\n",
    "\n",
    "endpoint = \"https://conductor.arcee.ai/v1\"\n",
    "\n",
    "model = \"auto\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of models from the API\n",
    "models = client.models.list()\n",
    "\n",
    "# Iterate over the models and print the details\n",
    "for m in models:\n",
    "    print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Write a short and friendly welcome message for a new user of Arcee Conductor, Arcee AI's inference platform.\"\"\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.9,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print_streaming_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain what put and call options are, and why they are used. Show a simple example.\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.9,\n",
    "    stream=True,\n",
    "    max_tokens=16384,\n",
    ")\n",
    "\n",
    "#for m in response:\n",
    "#    print(m.choices)\n",
    "\n",
    "print_streaming_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Explain the difference between logit-based distillation and hidden state distillation.\n",
    "       Show an example for both with Pytorch code, \n",
    "       with BERT-Large as the teacher model, and BERT-Base as the student model.\n",
    "       \"\"\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.9,\n",
    "    stream=True,\n",
    "    max_tokens=16384,\n",
    ")\n",
    "\n",
    "print_streaming_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Write a Python function that prints a streaming response from an OpenAI API call.\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.9,\n",
    "    stream=True,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "print_streaming_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "def print_streaming_response(response):\n",
    "    num_tokens = 0\n",
    "    content = \"\"\n",
    "    model_id = None\n",
    "    \n",
    "    for message in response:\n",
    "        if len(message.choices) > 0:\n",
    "            # Capture model ID from the first chunk if available\n",
    "            if model_id is None and hasattr(message, 'model'):\n",
    "                model_id = message.model\n",
    "                \n",
    "            num_tokens += 1\n",
    "            chunk = message.choices[0].delta.content\n",
    "            if chunk:\n",
    "                content += chunk\n",
    "                clear_output(wait=True)\n",
    "                display(Markdown(content))\n",
    "    \n",
    "    print(f\"\\n\\nNumber of tokens: {num_tokens}\")\n",
    "    if model_id:\n",
    "        print(f\"Model ID: {model_id}\")\n",
    "    else:\n",
    "        print(\"Model ID not available in response\")\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"auto\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Make this function more pythonic. Explain your changes. Code: {code}\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.9,\n",
    "    stream=True,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "print_streaming_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"alice.txt\") as file:\n",
    "    book_text1 = file.read()\n",
    "\n",
    "# Count the number of words\n",
    "num_words = len(book_text1.split())\n",
    "print(f\"Number of words: {num_words}\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Write a psychological profile of the main characters of this book. \n",
    "       Support your analysis with direct relevant quotes from the text.\n",
    "       \n",
    "       Book: {book_text1}\"\"\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.9,\n",
    "    stream=True,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "print_streaming_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-openai-client",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
